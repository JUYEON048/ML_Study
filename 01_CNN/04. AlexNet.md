## 04.AlexNet

> **AlexNet paper** </br>
> https://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf

<br/>

#### > Introduction
- 2012년 저명한 Computer Vision 대회인 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)에서 독보적으로 1등을하며, 딥 러닝 열풍에 큰 기여함.
- 기본 구조는 LeNet과크게 다르지 않음.
- 2개의 GPU로 병렬연산을 수행하기 위해서 병렬적인 구조로 설계되었다는 점이 가장 큰 변화.

<br/>

#### > 기본 구조
<br/>

>![](https://t1.daumcdn.net/cfile/tistory/99FEB93C5C80B5192E) </br>
> *AlexNet Architecture* </br>
> (https://t1.daumcdn.net/cfile/tistory/99FEB93C5C80B5192E) </br>
<br/>

- 22,000개 이상의 Categories를 가지는 1,500만개 이상의 ImageNet data 사용.
- 비선형 함수 ReLU 사용.
- Data Augmentation.
- SGD, 가중치 감소, 모멘텀 기술 사용.
- Dropout.
- Softmax 함수 적용, 1,000개의 클래스 확률 계산.   

<br/>
</br>

#### > ReLU Function
LeNet은 활성화 함수로 hyperbolic-tangent를 활용하였으나,AlexNet은 ReLU는를 활성화 홤수로 활용하였다.

- **활성화함수**(Activation function)  
: 뉴럴 네트워크의 개별 뉴런에 들어오는 입력신호의 총합을 출력신호로 변환하는 함수.  
: 퍼셉트론과 뉴럴 네트워크의 유일한 차이점이며, 활성화트함수는 대개 비선형함수(non-linear function)를 쓴다.  
(자세한 내용은 링크 참고, https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/)

<br/>

>![](https://imgs.developpaper.com/imgs/3329299216-5e720e8246afc_articlex.png)<br/>
>*활성화함수(Sigmoid, TanH, ReLU)* </br>
>(https://imgs.developpaper.com/imgs/3329299216-5e720e8246afc_articlex.png) </br>
<br/>

 **TanH**(hyperbolic-tangent)는 x가 -5보다 작거나, 5보다 크면 그래디언트가 0으로 작아지는 단점이 있다. 이에 반해, **ReLU**(Rectified Linear Unit) x가 양수이기만하면 그래디언트가 항상 1로 일정하다. 따라서 그래디언트가 0이 되는 현상을 피할 수 있고(x가 양수일때), 미분하기도 편리하여 계산복잡성이 낮다. 다만 0을 기준으로 대칭 구조가 아니며, x가 음수이면 그래디언트가 무조건 0이 되는 단점이 있다.

<br/>
</br>

#### > Local Response Normalization

   ReLU가 gradient saturation을 막는데 많은 도움이 되었지만, 아래와 같은 normalization도 도움이 되었다. 아래 식은 network의 ReLU를 빠져나온 출력들을 대상으로 간은 spatial위치 (x,y)의 인접한 kernel map n개를 모아서 정규화한다. 이는 생물학전 뉴런에서 강한 자극이 주변의 약한 자극을 막는 효과(lateral inhibition)을 모방한 일반화 효과이며, i는 커널 number, k/n/alpha/beta는 hyper parameter 이다. 

<br/>

>![](https://blog.kakaocdn.net/dn/0cXik/btqzAWXNLpy/fuMEgFoECmIYh7cbT5ex4k/img.png)<br/>
>*local response normalization* </br>
>(https://blog.kakaocdn.net/dn/0cXik/btqzAWXNLpy/fuMEgFoECmIYh7cbT5ex4k/img.png) </br>

<br/>
</br>

#### > Overlapped Pooling
 CNN에서 일반적으로 pooling은 convolution 연산을 통해 얻은 eature-map 영상의 크기를 줄이기 위하여 사용되며, average pooling, max poolin 등이 있다. LeNet에서는 averaged pooling 방식을 사용하였고, AlexNet에서는 max pooling을 사용하였다. 
 통상적으로 pooling을 할 때 겹치는 부분이 없게 하는것이 대부분이다. 따라서 poolilng window의 크기가 2x2라면, stride(건너뛰기)eh 2를 사용하여 출력 영상의 크기가 가로/세로 각 1/2로 줄어드는 효과를 얻는다. 그러나 AlexNet은 2x2 윈도우 대신 3x3 윈도우를 선택하고, stride를 2로하는 overlapped pooling방식을 사용했다.

<br/>
</br>

#### > Data Augmentation
- 256x256 size의 이미지를 224x224로 random crop한 뒤, translation/horizontal reflection를 진행함으로서 학습데이터를 2048배 증가시킴.
- 테스트 시에는 224x224 size의 이미지로 네 귀퉁이 + 가운데를 추출한 5장을 horizontal reflection한 이미지 10장의 softmax 결과를 앙상블하여 결과 도출.
- RGB pixel에 대한 PCA를 진행하여 augmentation하는 기법을 활용하여 color의 변화와 출렁임에도 유지하는 object identitiy를 학습할 수 있도록 함.
- 즉, 각 이미지 데이터를 reshape하여 R,G,B 세 개의 데이터로 변환하고, 각 R,G,B 컬럼의 principal vector, principal eigencalue에 약간의 random 요소를 더해서 RGB를 미세하게 같은 방향, 크기로 변동시키는 방법.

<br/>
<br/>
<br/>

>04.AlexNet 참고자료<br/>
>https://blog.naver.com/laonple/220654387455<br/>
>https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/<br/>
>https://dalpo0814.tistory.com/23
