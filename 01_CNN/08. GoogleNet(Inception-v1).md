## 08. GoogleNet(Inception-v1)

> **GoogleNet paper** </br>
> https://arxiv.org/abs/1409.4842 </br>

</br>
</br>



### 1. Introduction
>![Error Rate in ILSVRC 2014](https://cdn-images-1.medium.com/max/800/1*_X6NzAnveK3QBT029X8D7Q.png)</br>
>(https://cdn-images-1.medium.com/max/800/1*_X6NzAnveK3QBT029X8D7Q.png)</br>


- GoogleNet은 2014년 ImageNet에서 개최한 ILSVRC14(ImageNet Large-Scale Visual Recognition Challenge 2014)에서 VGG19를 제치고 1등을 수상.
- 19층의 VGG19보다 조금 더 깊은 22층으로 구성되어 있음.
- GoogleNet의 핵심 설계 철학은 주어진 하드웨어 자원을 최대한 효율적으로 이용하면서도 학습 능력은 극대화 할 수 있도록 깊고 넓은 망을 갖는 구조를 설계하는 것.

</br>
</br>
</br>

### 2. Architecture

> ![](https://cdn-images-1.medium.com/fit/t/1600/480/1*ZFPOSAted10TPd3hBQU8iQ.png)</br>
> (https://cdn-images-1.medium.com/fit/t/1600/480/1*ZFPOSAted10TPd3hBQU8iQ.png) </br>

> ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdtxVxa%2Fbtqzi3Yjhzq%2FRshkbka1XhAhvPQypxfi41%2Fimg.png)</br>
> (https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdtxVxa%2Fbtqzi3Yjhzq%2FRshkbka1XhAhvPQypxfi41%2Fimg.png)</br>

> ![](https://www.fatalerrors.org/images/blog/72be4758f3c492ab9736dc389faed6b1.jpg)</br>
> (https://www.fatalerrors.org/images/blog/72be4758f3c492ab9736dc389faed6b1.jpg)</br>

</br>
</br>
</br>

### 3.  1x1 Convolution
> ![1x1 Convolution](https://qph.fs.quoracdn.net/main-qimg-3d412cacb0435a8e56eda709ae26795f)</br>
> (https://qph.fs.quoracdn.net/main-qimg-3d412cacb0435a8e56eda709ae26795f)</br>


- 1x1 convolution을 하는 결정적인 이유는 차원을 줄이는 것(feature-map의 갯수를 줄이는 목적).
- 1x1 convolution을 수행하면, 여러 개의 feature-map으로부터 비슷한 성질을 갖는 것들을 묶어낼 수 있고, 결과적으로 feature-map의 숫자를 줄일 수 있으며, feature-map의 숫자가 줄어들게 되면 연산량을 줄일 수 있게 된다. **연산량이 줄어들게 되면, 신경망이 더 깊어질 수 있는 여지가 생긴다.**

</br>
</br>
</br>

### 4. Factorizing Convolutions
>![enter image description here](https://blog.kakaocdn.net/dn/59CI0/btqAPdxQrQQ/NIX745po8xCjtzMtMcvnGK/img.png)</br>
>(https://blog.kakaocdn.net/dn/59CI0/btqAPdxQrQQ/NIX745po8xCjtzMtMcvnGK/img.png)</br>

큰 필터 크기를 갖는 convolution 커널을 인수 분해 하면, 작은 커널 여러 개로 구성된 deep network를 만들 수 있으며, 이렇게 되면 **parameter의 수가 더 줄어들면서 망은 깊어지는 효과**를 얻을 수 있다.

</br>
</br>
</br>

### 5. Inception module

> ![Inception module](https://t1.daumcdn.net/cfile/tistory/993A9D4F5AD5813E11)</br>
> (https://t1.daumcdn.net/cfile/tistory/993A9D4F5AD5813E11)</br>

> ![Inception module](https://i.ytimg.com/vi/VxhSouuSZDY/maxresdefault.jpg) </br>
> (https://i.ytimg.com/vi/VxhSouuSZDY/maxresdefault.jpg)</br>



 GoogleNet에 실제로 사용된 모듈은 1x1 convolution이 포함된 (b) 모델이다. 이전 층에서 생성된 Feature-map을 1x1, 3x3, 5x5 convolution, 3x3 max pooling 하여 얻은 feature-map들을 모두 함께 쌓으므로, 보다 다양한 종류의 특성이 도출된다(1x1 conv로 인한 연산량 감소는 당연).

</br>
</br>
</br>

### 6. Global average pooling
>![FC vs Global average pooling](https://www.spiedigitallibrary.org/ContentImages/Journals/JEIME5/29/6/063010/FigureImages/JEI_29_6_063010_f002.png)</br>
>(https://www.spiedigitallibrary.org/ContentImages/Journals/JEIME5/29/6/063010/FigureImages/JEI_29_6_063010_f002.png)</br>



 AlexNet, VGGNet 등에서는 Fully Connected(FC) Layer들이 Network의 후반부에 연결되어 있는데, GoogleNet의 경우 이러한 FC Layer를 활용하지 않고 **Global Average Pooling**를 사용하였다. 

> **Global Average Pooling**
> 이전 layer에서 산출된 feature-map들을 각각 평균낸 것을 이어서 1차원 벡터를 만들어 주는 것.

Global average pooling을 사용하므로써 얻을 수 있는 장점은 **가중치의 갯수를 상당히 많이 감소할 수 있다는 것**이다.  global average pooling을 사용하면 가중치가 단 한개도 필요하지 않다.

</br>
</br>
</br>

 ### 7. Auxiliary classifier

> ![Auxiliary classifier](https://www.researchgate.net/profile/Wang-Su-Jeon/publication/315911462/figure/fig6/AS:506127924563973@1497681708427/GoogleNet-structure-and-auxiliary-classifier-units.png)</br>
> (https://www.researchgate.net/profile/Wang-Su-Jeon/publication/315911462/figure/fig6/AS:506127924563973@1497681708427/GoogleNet-structure-and-auxiliary-classifier-units.png)</br>

 네트워크의 깊이가 깊어지면 깊어질수록 vanishing gradient 문제를 피하기가 어렵다. 다시말해, 가중치를 훈련하는 과정에 역전파(back propagation)를 주로 활용하는데, 역전파과정에서 가중치를 업데이트하기위해 사용되는 gradient가 점점 작아져서 0이 되어버리는 것이다. 따라서 네트워크 내의 가중치들이 제대로 훈련되지 않는다. GoogleNet에서는 이러한 문제를 해결하기위ㅜ하여 네트워크 중간에 두 개의 보조 분류기(auxiliary classifier)를 달았다.

</br>
</br>

 > About Inception-v2,3,4 (현재까지는 ~ v4, Inception-ResNet v2가 있음) </br>
 > https://blog.naver.com/laonple/220716782369

</br>
</br>
</br>

>GoogleNet 참고자료 </br>
>https://ikkison.tistory.com/m/86 </br>
>https://bskyvision.com/539  </br>
>https://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220692793375 </br>
>https://blog.naver.com/laonple/220704822964 
>https://blog.naver.com/laonple/220710707354 
