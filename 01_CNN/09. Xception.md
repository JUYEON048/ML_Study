## 09. Xception 
> **Xception paper** </br>
> https://arxiv.org/abs/1610.02357

</br>
</br>

### 1.  **Introduction** 
- Google이 2017년에 발표한 모델.
- ResNet 등장 이후, 연구진들은 Conv layer의 구성을 조금씩 다르게 잡아보기 시작.
- Xception은 Inception(Google)을 근간으로 두고있음.
- Inception module이 여러 개의 Conv feature map을 Concat하여 사용했던 것을 응용.
- Depthwise Separable Convolution기법 역시 근간으로 둠.
- Inception보다 구조가 간단하고 성능이 좋아 활용도가 높음.

</br>
</br>
</br>

### 2. Modified Depthwise Separable Convolution
>![enter image description here](https://miro.medium.com/max/863/1*VvBTMkVRus6bWOqrK1SlLQ.png)</br>
>(https://miro.medium.com/max/863/1*VvBTMkVRus6bWOqrK1SlLQ.png)</br>


>![enter image description here](https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11063-020-10367-9/MediaObjects/11063_2020_10367_Fig1_HTML.png)</br>
>(https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11063-020-10367-9/MediaObjects/11063_2020_10367_Fig1_HTML.png)</br>

원조 Depthwise Separable Convolution은 총 2단계로 진행된다. 일반적인 Convolution과 결과는 같지만, 연산량을 1/9 수준으로 감소할 수있다.

**[1단계] Channel-wise nxn spatial convolution**

> 위 그림에서와 같이 input으로 5개의 channel이 들어오면 5개의 n x n convolution을 각각 진행하여 합친다.

**[2단계] Pointwise Convolution**

> 1x1 convolution으로, 채널의 개수를 줄이기 위한 방법으로 사용됨.

</br>
</br>
</br>

### 3. Xception's Depthwise Separable Convolution

- (연산의 순서) 원래는 depthwise 를 진행하고, pointwise 를 했는데, 이제는 pointwise -> depthwise 로 바꿈.
- (Non-Linearity의 유무) Inception 모델의 경우, 첫 연산 후에 non-linearity (ReLU)가 있지만, Xception은 중간에 ReLU non-linearity 를 적용하지 않dma.
- (Residual connection) Residual connection 이 거의 모든 Layer 에 있다 -> 없애고 실험해봤더니 있을때의 정확도가 훨씬 높았음. residual connection 이 굉장히 중요한 요소임.

</br>
</br>
</br>

### 4. Architecture 
> ![enter image description here](https://blog.kakaocdn.net/dn/cURENc/btqGdQ4oEj2/7kbxgeNBccVQSZMbYZn2Kk/img.png)</br>
> (https://blog.kakaocdn.net/dn/cURENc/btqGdQ4oEj2/7kbxgeNBccVQSZMbYZn2Kk/img.png)</br>

Entry, Middle, Exit의 3개 구조로 나뉩니다.

1.  Entry Flow
    -   모든 convolutional layer 다음에는 batch normalization.
    -   Residual Network 가 합쳐진 Inception Module 3번

2.  Middle Flow
    -   반복되는 단순한 모델: 필터의 개수와 width/height 는 바뀌지 않는다
    -   ReLU -> Separable Conv 8번 반복.


</br>
</br>
</br>


>Xception 참고자료 </br>
>https://junklee.tistory.com/111 </br>
>https://wansook0316.github.io/ds/dl/2020/09/07/computer-vision-12-Xception.html </br>
>https://gamer691.blogspot.com/2019/02/paper-review-xception-deep-learning.html </br>
>https://nbviewer.jupyter.org/github/Hyunjulie/KR-Reading-Image-Segmentation-Papers/blob/master/Xception%EC%84%A4%EB%AA%85%EA%B3%BC%20Pytorch%EA%B5%AC%ED%98%84.ipynb </br>

